{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Let's code XAI's Grok-1 Step-by-step in PyTorch\n",
        "\n",
        "The purpose of this guide is to illustrate the specific architecture choices implemented in Grok, which you will find are very similar to other open-sourced transformers such as the Llama, Mistral and Gemma series but with a few interesting differences. Check out the YouTube video where i walk through this colab notebook and explain everything step-by-step\n",
        "\n",
        "\\[![ERROR DISPLAYING IMAGE, CLICK HERE FOR VIDEO]\\(https://img.youtube.com/vi/WW7ZxaC3OtA/0.jpg)](https://www.youtube.com/watch?v=WW7ZxaC3OtA)\n",
        "\n",
        "This notebook guide is designed for beginners; if you already feel confident coding a transformer in pytorch on your own then i recommend instead skimming through the model.py file in the [github repo](https://github.com/evintunador/minGrok) to see what makes Grok unique compared to other open-sourced LLMs. By beginner, i mean someone who understands matrix/tensor multiplication, general deep learning concepts like what a loss function is, and is capable of looking up pytorch documentation on any given function that they don't recognize, but maybe isn't well versed on transformers specifically. For an even better beginner's guide that uses an outdated architecture, check out [Andrej Karpathy's video on how to build GPT2](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5014s) and then come back here to learn about the more up-to-date methods that Grok utilizes.\n",
        "\n",
        "Also, check out the original open-source release of Grok [here](https://github.com/xai-org/grok-1) \\(spoiler: it's too big for you to run locally).\n",
        "\n",
        "If you enjoy this guide, then check out [my analogous one for Google's Gemma Model](https://www.youtube.com/watch?v=WW7ZxaC3OtA)\n",
        "\n",
        "**Note:** It's very easy to convince yourself that you understand something after watching a youtube video about it, but chances are you don't actually understand unless you can code it from scratch on your own. I highly recommend you mess around with this notebook and try to build your own minGrok from scratch\n",
        "\n",
        "# What this guide does NOT include\n",
        "The focus here is on architecture rather than optimization techniques, distributed training/inference, quantization, etc. As such, there are many parts of the original Grok repo that will not be included:\n",
        "- quantization\n",
        "- kv caching\n",
        "- the based twitter data Grok was trained on\n",
        "- the original tokenizer\n",
        "- activation sharding\n",
        "- jax code (pytorch is generally more well-known, but the difference isn't a big deal)\n",
        "- batched inference\n",
        "- the original parameter initialization distributions (i don't believe this knowledge has been shared)\n",
        "- other stuff i'm prolly forgetting\n",
        "\n",
        "# Table of Contents (I don't think the links work in google colab)\n",
        "1. [Spelled out walkthrough of every single tensor operation](#one)\n",
        "  \n",
        "  1a. [Boring setup stuff / Initializing the first residual state](#a)\n",
        "  \n",
        "  1b. [Normalization](#b)\n",
        "  \n",
        "  1c. [Initializing Multi-Query Attention](#c)\n",
        "  \n",
        "  1d. [Rotary Position Embeddings](#d)\n",
        "  \n",
        "  1e. [Calculating Self-Attention](#e)\n",
        "  \n",
        "  1f. [Our first residual connection](#f)\n",
        "  \n",
        "  1g. [Initializing the Mixture of Experts Feedforward Network](#g)\n",
        "\n",
        "  1h. [Input-dependent routing](#h)\n",
        "\n",
        "  1i. [Letting our experts do their thing](#i)\n",
        "\n",
        "  1j. [Applying our router to the output of our experts](#j)\n",
        "\n",
        "  1k. [Our final residual connection](#k)\n",
        "\n",
        "  1l. [Output](#l)\n",
        "\n",
        "2. [Actually functional model code](#two)\n",
        "\n",
        "  2a. [Multi-query attention](#twoa)\n",
        "\n",
        "  2b. [Mixture-of-Experts Feedforward Network](#twob)\n",
        "\n",
        "  2c. [Residual Layers](#twoc)\n",
        "\n",
        "  2d. [The full model](#twod)\n",
        "\n",
        "3. [Train and test your own minGrok (or load mine)](#three)\n",
        "\n",
        "  3a. [Setup](#threea)\n",
        "\n",
        "  3b. [Training your own](#threeb)\n",
        "\n",
        "  3c. [Alternatively, you can load the 1m parameter model I already trained](#threec)\n",
        "\n",
        "  3d. [Testing (performing inference)](#threed)"
      ],
      "metadata": {
        "id": "4WUJqR8FeXSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Spelled out walkthrough of every single tensor operation\n",
        "<a id='one'></a>\n",
        "In this section we'll walk through every important operation that Grok's architecture carries out using laughably small tensors. We've chosen tensors so small so that if you want to, you can literally pull out a calculator to 100% ensure you undersand what's happening. we'll begin with basic imports and whatnot\n",
        "\n",
        "### 1a. Boring setup stuff / Initializing the first residual state\n",
        "<a id='a'></a>"
      ],
      "metadata": {
        "id": "x-nuFaBitYeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "JOHHIHcjeWzN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary length. Grok's real vocab size is 131,072. Here let's just use an absurdly small number\n",
        "v = 10\n",
        "\n",
        "# Grok's maximum sequence length is 8192\n",
        "seq_len = 5\n",
        "\n",
        "# we'll use a batch size of 1 for simplicity when visualizing our tensors\n",
        "b = 1\n",
        "\n",
        "# now let's make ourselves a list of token indices\n",
        "tokens = torch.randint(v, (b, seq_len))\n",
        "tokens.shape, tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEje5kPPeW1Y",
        "outputId": "1caa289f-3702-4a16-cfb5-eed03c0e4fb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5]), tensor([[5, 1, 2, 9, 8]]))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# our embedding dimension. grok's is 6,144\n",
        "d = 8\n",
        "\n",
        "# initializing our token embedding matrix\n",
        "embedding = nn.Embedding(v, d)\n",
        "embedding.weight.shape, embedding.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BItItwY2eW3m",
        "outputId": "92aeb44f-8b7d-4645-9c5d-570f5eccefe3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 8]),\n",
              " Parameter containing:\n",
              " tensor([[ 0.7412,  0.0547,  3.2145,  1.4545, -0.0927, -0.9935,  1.2472, -0.2418],\n",
              "         [ 0.7663, -0.5664,  0.6916,  1.1916,  0.0204,  1.0723, -0.0458,  1.0531],\n",
              "         [-1.2627,  0.5304, -0.7493,  0.6560, -0.8619,  0.3624,  0.0518,  0.0111],\n",
              "         [ 0.5155,  0.9417,  1.3818,  0.0685, -0.3742, -0.3073, -0.4027, -1.5268],\n",
              "         [ 1.0571,  0.5966, -0.8332,  0.0296, -0.0792,  1.1656, -1.1542, -0.3768],\n",
              "         [-0.1968, -2.2025, -0.4323,  0.6378,  0.3747, -0.4296,  2.6077, -2.7697],\n",
              "         [-0.1418, -0.3240,  0.9460, -0.3286,  1.0215, -1.9522, -1.3987, -1.7352],\n",
              "         [-1.2985, -1.9296,  0.7410, -1.0136, -0.0223,  0.0184, -0.5538, -2.2284],\n",
              "         [ 1.7609, -1.0608,  0.6641, -1.2204,  0.8250,  0.5540,  1.8383,  1.0159],\n",
              "         [-0.6304, -1.3135, -0.7225,  0.6931,  0.2269, -0.2311, -0.3528, -0.0979]],\n",
              "        requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding our sequence of token indices\n",
        "x = embedding(tokens)\n",
        "x.shape, x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqKNqorqSshT",
        "outputId": "6b32fc31-f47c-44ad-d383-ee781951ed11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 8]),\n",
              " tensor([[[-0.1968, -2.2025, -0.4323,  0.6378,  0.3747, -0.4296,  2.6077,\n",
              "           -2.7697],\n",
              "          [ 0.7663, -0.5664,  0.6916,  1.1916,  0.0204,  1.0723, -0.0458,\n",
              "            1.0531],\n",
              "          [-1.2627,  0.5304, -0.7493,  0.6560, -0.8619,  0.3624,  0.0518,\n",
              "            0.0111],\n",
              "          [-0.6304, -1.3135, -0.7225,  0.6931,  0.2269, -0.2311, -0.3528,\n",
              "           -0.0979],\n",
              "          [ 1.7609, -1.0608,  0.6641, -1.2204,  0.8250,  0.5540,  1.8383,\n",
              "            1.0159]]], grad_fn=<EmbeddingBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b. Normalization\n",
        "<a id='b'></a>\n",
        "\n",
        "before normalizing, we need to scale our embeddings. Scaling embeddings has become common for post-GPT2 models. For example, both XAI's Grok and Google's Gemma scale by $\\sqrt{d}$. For our normalization we'll be using RMSNorm, a common technique that places vectors onto hyperspheres of radius $\\sqrt{d}$"
      ],
      "metadata": {
        "id": "MGoOnErjYiW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import sqrt\n",
        "\n",
        "# defining our embedding scaling parameter\n",
        "embedding_multiplier_scale = sqrt(d)\n",
        "# Grok's is defined as 78.38367176906169 which is the square root of its embedding dimension 6,144\n",
        "\n",
        "# then do the actual scaling\n",
        "x *= embedding_multiplier_scale\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou8IiJudSsj4",
        "outputId": "13b95a4b-1e30-4e7c-c551-7ac2d18baff9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5565, -6.2297, -1.2228,  1.8039,  1.0599, -1.2152,  7.3756,\n",
              "          -7.8340],\n",
              "         [ 2.1674, -1.6021,  1.9562,  3.3705,  0.0576,  3.0330, -0.1296,\n",
              "           2.9787],\n",
              "         [-3.5716,  1.5002, -2.1193,  1.8554, -2.4379,  1.0250,  0.1466,\n",
              "           0.0314],\n",
              "         [-1.7830, -3.7150, -2.0436,  1.9603,  0.6416, -0.6538, -0.9978,\n",
              "          -0.2770],\n",
              "         [ 4.9805, -3.0003,  1.8785, -3.4518,  2.3335,  1.5671,  5.1994,\n",
              "           2.8734]]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we'll perform our first normalization and keep our original x separate for use in the residual connection later\n",
        "# RMSNorm is a common normalization technique that places vectors on a hypersphere with radius sqrt(d)\n",
        "\n",
        "# first we square each entry in x and then take the mean of those values across each embedding vector\n",
        "mean_squared = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "mean_squared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-Oz0Gw2Z7qd",
        "outputId": "6afc82a8-a9d0-46ac-fd5e-f8a6f61029fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[20.2799],\n",
              "         [ 5.0679],\n",
              "         [ 3.7446],\n",
              "         [ 3.3639],\n",
              "         [11.5552]]], grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then we multiply x by the reciprocal of the square roots of mean_squared\n",
        "# 1e-5 is a very small number added for stability just in case an entry happens to be equal to 0 (since you can't divide by 0)\n",
        "x_normed = x * torch.rsqrt(mean_squared + 1e-5)\n",
        "x_normed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE0hSKbSa06u",
        "outputId": "ddcb2e35-d165-4926-83cd-b16630859a73"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1236, -1.3834, -0.2715,  0.4006,  0.2354, -0.2698,  1.6378,\n",
              "          -1.7396],\n",
              "         [ 0.9628, -0.7117,  0.8690,  1.4972,  0.0256,  1.3473, -0.0576,\n",
              "           1.3232],\n",
              "         [-1.8457,  0.7753, -1.0952,  0.9588, -1.2598,  0.5297,  0.0757,\n",
              "           0.0162],\n",
              "         [-0.9721, -2.0255, -1.1142,  1.0688,  0.3498, -0.3564, -0.5440,\n",
              "          -0.1510],\n",
              "         [ 1.4652, -0.8826,  0.5526, -1.0154,  0.6865,  0.4610,  1.5296,\n",
              "           0.8453]]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally, we multiply by a learnable scale parameter\n",
        "# This scale is initialized to 1's but if we were to train in this tutorial then it would change from 1's\n",
        "rms_scale = torch.ones(d)\n",
        "x_normed *= rms_scale\n",
        "x_normed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_Z4LQQnbuNG",
        "outputId": "720a56fd-e6a4-4331-8fdc-4f4e54c28f2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1236, -1.3834, -0.2715,  0.4006,  0.2354, -0.2698,  1.6378,\n",
              "          -1.7396],\n",
              "         [ 0.9628, -0.7117,  0.8690,  1.4972,  0.0256,  1.3473, -0.0576,\n",
              "           1.3232],\n",
              "         [-1.8457,  0.7753, -1.0952,  0.9588, -1.2598,  0.5297,  0.0757,\n",
              "           0.0162],\n",
              "         [-0.9721, -2.0255, -1.1142,  1.0688,  0.3498, -0.3564, -0.5440,\n",
              "          -0.1510],\n",
              "         [ 1.4652, -0.8826,  0.5526, -1.0154,  0.6865,  0.4610,  1.5296,\n",
              "           0.8453]]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's turn that RMSNorm into a function that we'll be able to reuse repeatedly later\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, use_scale=True):\n",
        "        super(RMSNorm, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = nn.Parameter(torch.ones(num_features)) if use_scale else None\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Calculate the mean squared value for each feature\n",
        "        mean_squared = inputs.pow(2).mean(dim=-1, keepdim=True)\n",
        "\n",
        "        # Normalize inputs\n",
        "        normed_inputs = inputs * torch.rsqrt(mean_squared + self.eps)\n",
        "\n",
        "        # Apply scale if it exists\n",
        "        if self.scale is not None:\n",
        "            normed_inputs = normed_inputs * self.scale\n",
        "\n",
        "        return normed_inputs"
      ],
      "metadata": {
        "id": "L7biEvzBSsnr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1c. Initializing Multi-Query Attention\n",
        "<a id='c'></a>\n",
        "[multi-query attention](https://arxiv.org/abs/1911.02150) is the de facto standard for saving on parameter counts in order to get a bigger model. The idea is that the model can make multiple queries to the residual state and have those many queries be answered by shared keys & values."
      ],
      "metadata": {
        "id": "b5gp8C6hb2tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first up, remember we're currently working with two separate objects\n",
        "# x is for the residual connection and x_normed will go into our Attention calculation\n",
        "x, x_normed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpQZkXhLc9RJ",
        "outputId": "fc74ad60-f529-4b5e-ba8d-91ed974ff065"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-0.5565, -6.2297, -1.2228,  1.8039,  1.0599, -1.2152,  7.3756,\n",
              "           -7.8340],\n",
              "          [ 2.1674, -1.6021,  1.9562,  3.3705,  0.0576,  3.0330, -0.1296,\n",
              "            2.9787],\n",
              "          [-3.5716,  1.5002, -2.1193,  1.8554, -2.4379,  1.0250,  0.1466,\n",
              "            0.0314],\n",
              "          [-1.7830, -3.7150, -2.0436,  1.9603,  0.6416, -0.6538, -0.9978,\n",
              "           -0.2770],\n",
              "          [ 4.9805, -3.0003,  1.8785, -3.4518,  2.3335,  1.5671,  5.1994,\n",
              "            2.8734]]], grad_fn=<MulBackward0>),\n",
              " tensor([[[-0.1236, -1.3834, -0.2715,  0.4006,  0.2354, -0.2698,  1.6378,\n",
              "           -1.7396],\n",
              "          [ 0.9628, -0.7117,  0.8690,  1.4972,  0.0256,  1.3473, -0.0576,\n",
              "            1.3232],\n",
              "          [-1.8457,  0.7753, -1.0952,  0.9588, -1.2598,  0.5297,  0.0757,\n",
              "            0.0162],\n",
              "          [-0.9721, -2.0255, -1.1142,  1.0688,  0.3498, -0.3564, -0.5440,\n",
              "           -0.1510],\n",
              "          [ 1.4652, -0.8826,  0.5526, -1.0154,  0.6865,  0.4610,  1.5296,\n",
              "            0.8453]]], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define the hyperparameters of MQA\n",
        "num_q_heads = 2 # Grok has 48 query heads per layer\n",
        "num_kv_heads = 1 # Grok uses 8 key and value heads per layer\n",
        "assert num_q_heads % num_kv_heads == 0 # each q needs to match up to a kv\n",
        "\n",
        "# Grok attention head matrices have a size of 128 which is common for compatibility with FlashAttention (which we're not doing here)\n",
        "head_size = 4"
      ],
      "metadata": {
        "id": "eDl4ypypdK4C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we'll initialize our self-attention weight matrices\n",
        "Wq = nn.Linear(d, num_q_heads * head_size, bias=False)\n",
        "Wk = nn.Linear(d, num_kv_heads * head_size, bias=False)\n",
        "Wv = nn.Linear(d, num_kv_heads * head_size, bias=False)\n",
        "print(\"Attention weights: \", Wq.weight.shape, Wk.weight.shape, Wv.weight.shape)\n",
        "\n",
        "# and project x_normed out to get our queries, keys and values\n",
        "Xq = Wq(x_normed)\n",
        "Xk = Wk(x_normed)\n",
        "Xv = Wv(x_normed)\n",
        "print(\"Attention projections: \", Xq.shape, Xk.shape, Xv.shape)\n",
        "\n",
        "# then reshape them to separate out by head\n",
        "Xq = Xq.view(b, -1, num_q_heads, head_size)\n",
        "Xk = Xk.view(b, -1, num_kv_heads, head_size)\n",
        "Xv = Xv.view(b, -1, num_kv_heads, head_size)\n",
        "print(\"Reshaped: \", Xq.shape, Xk.shape, Xv.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH0rZAHnffMn",
        "outputId": "a9fef455-3aaa-40ab-a4ee-43137964fcef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:  torch.Size([8, 8]) torch.Size([4, 8]) torch.Size([4, 8])\n",
            "Attention projections:  torch.Size([1, 5, 8]) torch.Size([1, 5, 4]) torch.Size([1, 5, 4])\n",
            "Reshaped:  torch.Size([1, 5, 2, 4]) torch.Size([1, 5, 1, 4]) torch.Size([1, 5, 1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1d. Rotary Position Embeddings\n",
        "<a id='d'></a>\n",
        "Grok uses [Rotary Positional Embeddings (RoPE)](https://arxiv.org/abs/2104.09864) to provide information about the order of tokens in the sequence since attention is inherently blind to the order of the tokens. We won't go in-depth on how/why RoPE works, but the basic idea is that it \"rotates\" the rows in the queries and keys using trigonometry functions, and this rotation helps the model learn how far two given tokens are from one another"
      ],
      "metadata": {
        "id": "85zBiEBikLB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RoPE setup\n",
        "assert head_size % 2 == 0\n",
        "# this is a hyperparameter of RoPE that manipulates the frequency of the trig functions. I've only ever seen 10,000 be used\n",
        "theta = 10000"
      ],
      "metadata": {
        "id": "R8SWYgJRSsl8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dynamically compute frequency cis based on the input sequence length\n",
        "exponents = torch.arange(0, head_size, 2)\n",
        "freqs = 1.0 / (theta ** (exponents.float() / head_size))\n",
        "t = torch.arange(seq_len)\n",
        "freqs = torch.outer(t, freqs).float()\n",
        "freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "# Apply rotary embeddings to our query\n",
        "Xq = torch.view_as_complex(torch.stack(torch.chunk(Xq.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
        "Xq = torch.view_as_real(Xq * freqs_cis.unsqueeze(0)).type_as(Xq)  # Ensure batch dimension is handled\n",
        "Xq = torch.cat(torch.chunk(Xq, 2, dim=-1), dim=-2)\n",
        "Xq = Xq.reshape(Xq.shape[0], Xq.shape[1], Xq.shape[2], -1).transpose(1, 2)\n",
        "\n",
        "# and then to our key\n",
        "Xk = torch.view_as_complex(torch.stack(torch.chunk(Xk.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
        "Xk = torch.view_as_real(Xk * freqs_cis.unsqueeze(0)).type_as(Xk)  # Ensure batch dimension is handled\n",
        "Xk = torch.cat(torch.chunk(Xk, 2, dim=-1), dim=-2)\n",
        "Xk = Xk.reshape(Xq.shape[0], Xk.shape[1], Xk.shape[2], -1).transpose(1, 2)\n",
        "\n",
        "Xq.shape, Xq.dtype, Xk.shape, Xk.dtype"
      ],
      "metadata": {
        "id": "6od2ht7CSspq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3d3102-2be5-4602-ade9-20033d70a4e0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 2, 4]),\n",
              " torch.complex64,\n",
              " torch.Size([1, 5, 1, 4]),\n",
              " torch.complex64)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1e. Calculating Self-Attention\n",
        "<a id='e'></a>\n",
        "now we get to perform the actual attention calculation. Skip to the normalization before the softmax if you just want to see what Grok does differently here"
      ],
      "metadata": {
        "id": "rvpeoAFRt51t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
        "if num_kv_heads != num_q_heads:\n",
        "  num_queries_per_kv = num_q_heads // num_kv_heads\n",
        "  Xk = torch.repeat_interleave(Xk, num_queries_per_kv, dim=2)\n",
        "  Xv = torch.repeat_interleave(Xv, num_queries_per_kv, dim=2)\n",
        "\n",
        "Xq.shape, Xk.shape, Xv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVeOQpzrs1Ej",
        "outputId": "f2f505f7-c6d4-4072-e20d-9e97af3e1e91"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 2, 4]), torch.Size([1, 5, 2, 4]), torch.Size([1, 5, 2, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
        "Xq = Xq.transpose(1, 2)\n",
        "Xk = Xk.transpose(1, 2)\n",
        "Xv = Xv.transpose(1, 2)\n",
        "\n",
        "Xq.shape, Xk.shape, Xv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmOLwaqDs1MS",
        "outputId": "17c37fc5-ac38-4c40-c189-707ef4c9bd13"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 2, 5, 4]), torch.Size([1, 2, 5, 4]), torch.Size([1, 2, 5, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculates attention logits by performing a batch matrix multiplication between queries and keys\n",
        "attn_logits = torch.matmul(Xq, Xk.transpose(2, 3)).type_as(Xv)\n",
        "\n",
        "# then we scale the logits. If anyone knows why they use 0.08838834764831845 in Grok please lmk. Maybe it's a learned value?\n",
        "attn_logits *= 0.08838834764831845\n",
        "# scaling the scores down (0.088 is less than 1, hence \"down\") has the effect of making the upcoming softmax distribution flatter\n",
        "\n",
        "attn_logits.shape, attn_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CCcntgts1fG",
        "outputId": "3517f570-1140-4aef-ce50-3f463252c3b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-b1ccba4761f0>:2: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
            "  attn_logits = torch.matmul(Xq, Xk.transpose(2, 3)).type_as(Xv)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 2, 5, 5]),\n",
              " tensor([[[[ 0.0553, -0.0212,  0.0116,  0.1514, -0.0976],\n",
              "           [-0.0226, -0.0130, -0.0256, -0.0273, -0.0170],\n",
              "           [ 0.0590,  0.0702,  0.0909,  0.1302,  0.0244],\n",
              "           [ 0.0324,  0.0074,  0.0121,  0.0661, -0.0083],\n",
              "           [-0.0838,  0.0567,  0.0263, -0.0987,  0.0289]],\n",
              " \n",
              "          [[ 0.0175, -0.0485, -0.0195, -0.0117, -0.0180],\n",
              "           [ 0.0008, -0.0336, -0.0260,  0.0207, -0.0558],\n",
              "           [ 0.0356,  0.0046,  0.0468,  0.0739, -0.0298],\n",
              "           [ 0.0617,  0.0697,  0.0974,  0.1338,  0.0217],\n",
              "           [-0.0894,  0.0544, -0.0068, -0.1604,  0.0939]]]],\n",
              "        grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normally before performing the softmax operation you'd normalize your attn_logits by dividing by the square root of the head dimension, but in Grok they go about pre-softmax normalization in a way that I haven't seen before"
      ],
      "metadata": {
        "id": "jLrwvhO_3_TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we'll scale and clip our attention logits\n",
        "# the tanh is a nonlinear function that pushes all of the entries in scores into the range (-1, 1)\n",
        "# then they're scaled up to the range (-30, 30). The number 30 is an arbitrary choice\n",
        "# the purpose of this scaling is to regularize and prevent numerical stability that might otherwise mess with the upcoming softmax\n",
        "max_attn_val = torch.tensor(30.0, dtype = attn_logits.dtype)\n",
        "attn_logits = max_attn_val * torch.tanh(attn_logits / max_attn_val)\n",
        "\n",
        "attn_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H77RY1-awjmv",
        "outputId": "2016208a-49a0-4b89-d21c-bb512ddd2a04"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.0553, -0.0212,  0.0116,  0.1514, -0.0976],\n",
              "          [-0.0226, -0.0130, -0.0256, -0.0273, -0.0170],\n",
              "          [ 0.0590,  0.0702,  0.0909,  0.1302,  0.0244],\n",
              "          [ 0.0324,  0.0074,  0.0121,  0.0661, -0.0083],\n",
              "          [-0.0838,  0.0567,  0.0263, -0.0987,  0.0289]],\n",
              "\n",
              "         [[ 0.0175, -0.0485, -0.0195, -0.0117, -0.0180],\n",
              "          [ 0.0008, -0.0336, -0.0260,  0.0207, -0.0558],\n",
              "          [ 0.0356,  0.0046,  0.0468,  0.0739, -0.0298],\n",
              "          [ 0.0617,  0.0697,  0.0974,  0.1338,  0.0217],\n",
              "          [-0.0894,  0.0544, -0.0068, -0.1604,  0.0939]]]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mask tensor with shape [batch_size, num_heads, seq_len, seq_len]\n",
        "# The lower-triangular 1's allow the softmax to view those outputs and the 0's prevent each token from viewing future tokens\n",
        "mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.uint8)).view(1, 1, seq_len, seq_len)\n",
        "# Expand the mask to cover the batch size and number of heads\n",
        "mask = mask.expand(b, num_q_heads, -1, -1)  # The mask now has shape [b, num_heads, seq_len, seq_len]\n",
        "\n",
        "# Convert the mask to a boolean tensor\n",
        "mask = mask.to(dtype=torch.bool)\n",
        "\n",
        "# Use a very large negative number for masked positions.\n",
        "# This large number will be turned into effectively 0 probability by the softmax later\n",
        "attn_logits = torch.where(mask, attn_logits, torch.tensor(-1e30, device=attn_logits.device, dtype=attn_logits.dtype))\n",
        "\n",
        "attn_logits.shape, attn_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPmJK1zbwjuS",
        "outputId": "88396085-e2c0-451b-8f47-aa18ffbb7adc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 2, 5, 5]),\n",
              " tensor([[[[ 5.5275e-02, -1.0000e+30, -1.0000e+30, -1.0000e+30, -1.0000e+30],\n",
              "           [-2.2552e-02, -1.2955e-02, -1.0000e+30, -1.0000e+30, -1.0000e+30],\n",
              "           [ 5.8986e-02,  7.0166e-02,  9.0920e-02, -1.0000e+30, -1.0000e+30],\n",
              "           [ 3.2447e-02,  7.3974e-03,  1.2095e-02,  6.6057e-02, -1.0000e+30],\n",
              "           [-8.3790e-02,  5.6658e-02,  2.6258e-02, -9.8691e-02,  2.8925e-02]],\n",
              " \n",
              "          [[ 1.7497e-02, -1.0000e+30, -1.0000e+30, -1.0000e+30, -1.0000e+30],\n",
              "           [ 7.8953e-04, -3.3578e-02, -1.0000e+30, -1.0000e+30, -1.0000e+30],\n",
              "           [ 3.5604e-02,  4.5637e-03,  4.6832e-02, -1.0000e+30, -1.0000e+30],\n",
              "           [ 6.1658e-02,  6.9654e-02,  9.7441e-02,  1.3384e-01, -1.0000e+30],\n",
              "           [-8.9426e-02,  5.4441e-02, -6.7987e-03, -1.6040e-01,  9.3887e-02]]]],\n",
              "        grad_fn=<WhereBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we perform the softmax operation\n",
        "attn_logits = nn.Softmax(dim=-1)(attn_logits)\n",
        "attn_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knsd9x-iwjxd",
        "outputId": "9a6868bc-92ba-476a-b1b2-921ad6e996e3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.4976, 0.5024, 0.0000, 0.0000, 0.0000],\n",
              "          [0.3285, 0.3322, 0.3392, 0.0000, 0.0000],\n",
              "          [0.2507, 0.2445, 0.2456, 0.2592, 0.0000],\n",
              "          [0.1862, 0.2142, 0.2078, 0.1834, 0.2084]],\n",
              "\n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.5086, 0.4914, 0.0000, 0.0000, 0.0000],\n",
              "          [0.3355, 0.3252, 0.3393, 0.0000, 0.0000],\n",
              "          [0.2428, 0.2447, 0.2516, 0.2609, 0.0000],\n",
              "          [0.1861, 0.2149, 0.2021, 0.1733, 0.2235]]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then matmul by our value projection\n",
        "output = torch.matmul(attn_logits, Xv)\n",
        "output.shape, output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtEbx0D1wj0K",
        "outputId": "cf951ad9-a9a3-4b9d-f932-d93557081192"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 2, 5, 4]),\n",
              " tensor([[[[ 0.3620,  0.0844, -0.0202, -1.1810],\n",
              "           [ 0.2152, -0.4033, -0.3436, -0.1793],\n",
              "           [ 0.1427, -0.0835, -0.0616, -0.4235],\n",
              "           [ 0.1757, -0.1843, -0.1164, -0.4702],\n",
              "           [ 0.1522, -0.1471, -0.2192, -0.2813]],\n",
              " \n",
              "          [[ 0.3620,  0.0844, -0.0202, -1.1810],\n",
              "           [ 0.2184, -0.3926, -0.3365, -0.2012],\n",
              "           [ 0.1447, -0.0767, -0.0570, -0.4374],\n",
              "           [ 0.1733, -0.1828, -0.1139, -0.4671],\n",
              "           [ 0.1512, -0.1459, -0.2289, -0.2654]]]],\n",
              "        grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and reshape to put the sequence length back into place and the outputs of our heads lined up\n",
        "output = output.transpose(1, 2).contiguous().view(b, seq_len, -1)\n",
        "output.shape, output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bpSgyBA41vV",
        "outputId": "6aa7ee1c-9a0e-4a7c-84f8-358346f0aac7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 8]),\n",
              " tensor([[[ 0.3620,  0.0844, -0.0202, -1.1810,  0.3620,  0.0844, -0.0202,\n",
              "           -1.1810],\n",
              "          [ 0.2152, -0.4033, -0.3436, -0.1793,  0.2184, -0.3926, -0.3365,\n",
              "           -0.2012],\n",
              "          [ 0.1427, -0.0835, -0.0616, -0.4235,  0.1447, -0.0767, -0.0570,\n",
              "           -0.4374],\n",
              "          [ 0.1757, -0.1843, -0.1164, -0.4702,  0.1733, -0.1828, -0.1139,\n",
              "           -0.4671],\n",
              "          [ 0.1522, -0.1471, -0.2192, -0.2813,  0.1512, -0.1459, -0.2289,\n",
              "           -0.2654]]], grad_fn=<ViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finally we can initialize and apply our output projection that mixes the information from the heads together\n",
        "Wout = nn.Linear(num_q_heads * head_size, d, bias=False)\n",
        "Xout = Wout(output)\n",
        "Xout.shape, Xout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paC0J9h549KI",
        "outputId": "f5d9e908-6dc7-414c-fb7a-0ba7ce680742"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 8]),\n",
              " tensor([[[ 0.0343,  0.0630,  0.5592,  0.2810, -0.4654, -0.0534,  0.3208,\n",
              "           -0.2713],\n",
              "          [ 0.0990, -0.0362,  0.2376,  0.1141, -0.0208, -0.0466, -0.0711,\n",
              "           -0.1092],\n",
              "          [ 0.0309,  0.0135,  0.2291,  0.1029, -0.1729, -0.0093,  0.0933,\n",
              "           -0.1153],\n",
              "          [ 0.0471, -0.0023,  0.2723,  0.1178, -0.1851, -0.0064,  0.0755,\n",
              "           -0.1347],\n",
              "          [ 0.0756,  0.0228,  0.2120,  0.0981, -0.0367, -0.0415,  0.0029,\n",
              "           -0.0631]]], grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1f. Our first residual connection\n",
        "<a id='f'></a>\n",
        "Here we'll normalize the output of our attention mechanism and then add it to our residual state"
      ],
      "metadata": {
        "id": "IFdEv7wg6Vm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "post_attn_norm = RMSNorm(d)\n",
        "x += post_attn_norm(Xout)\n",
        "x.shape, x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdVEPPxi6hcc",
        "outputId": "ef3c5a94-dc7a-4757-8b2f-5520ccbff04e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 8]),\n",
              " tensor([[[-0.4477, -6.0295,  0.5536,  2.6965, -0.4184, -1.3849,  8.3945,\n",
              "           -8.6957],\n",
              "          [ 3.0517, -1.9253,  4.0788,  4.3899, -0.1285,  2.6171, -0.7643,\n",
              "            2.0030],\n",
              "          [-3.3151,  1.6124, -0.2189,  2.7086, -3.8719,  0.9475,  0.9206,\n",
              "           -0.9252],\n",
              "          [-1.4373, -3.7323, -0.0449,  2.8253, -0.7168, -0.7011, -0.4434,\n",
              "           -1.2659],\n",
              "          [ 5.8011, -2.7530,  4.1791, -2.3878,  1.9352,  1.1172,  5.2306,\n",
              "            2.1884]]], grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then we'll normalize the current state of our residual for use in our MoE later\n",
        "pre_moe_norm = RMSNorm(d)\n",
        "x_normed = pre_moe_norm(x)\n",
        "# so now we're working with x, which we'll use later for our next residual conenction, and x_normed which is used by our MoE MLP"
      ],
      "metadata": {
        "id": "Anfh-ziI6hhk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1g. Initializing the Mixture of Experts Feedforward Network\n",
        "<a id='g'></a>\n",
        "Mixture of Experts is a very old idea in machine learning that became popular in transformers with [Mistral 7b](https://github.com/mistralai/mistral-src) and supposedly GPT4. The idea is that instead of having every single parameter in the model contribute to the output, we allow some parameters to only come into effect based on their \"expertise.\" In practice what this means is that we instantiate multiple different traditional 2-layer feedforward networks with nonlinearities (\"experts\"), and then create a router that learns when to dynamically utilize different experts on a per-token basis. Usually multiple experts are selected by means of top-k choices where k is a hyperparameter designating how many experts to utilize (2 is the most common choice), however [it's also possible to implement a dynamic number of experts by using top-p instead](https://arxiv.org/pdf/2403.07652.pdf)"
      ],
      "metadata": {
        "id": "RUldQC41DpQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define the hyperparameters of our MoE & FFN\n",
        "tot_num_experts = 4 # Grok has 8 experts\n",
        "chosen_num_experts = 2 # Grok also uses its top 2 experts\n",
        "widening_factor = 2 # Grok uses a widening factor of 8 rather than 2"
      ],
      "metadata": {
        "id": "m6DX_mGRaSRK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first, let's define what an expert looks like\n",
        "# Rather than writing the same code multiple times we'll actually define an expert class and then instantiate multiple versions of it\n",
        "class Expert(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim * 2, bias=False)  # Double the output for gating\n",
        "        self.layer2 = nn.Linear(hidden_dim, output_dim, bias=False)  # Output layer remains the same\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Implements a 2-layer GeGLU feedforward network. https://arxiv.org/pdf/2002.05202v1.pdf\"\"\"\n",
        "\n",
        "        # Split the output of the first layer for gating\n",
        "        x, gate = self.layer1(x).chunk(2, dim=-1)\n",
        "\n",
        "        # Apply GeLU to the gate, and then multiply element-wise\n",
        "        x = F.gelu(gate) * x\n",
        "        x = self.layer2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "sLThJV8f6hj6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a List of 4 Expert modules\n",
        "experts = nn.ModuleList([Expert(input_dim = d, hidden_dim = d * widening_factor, output_dim = d) for _ in range(tot_num_experts)])\n",
        "\n",
        "# we'll print out the pieces of the first expert for you to visualize\n",
        "experts[0].layer1.weight.shape, experts[0].layer1.weight, experts[0].layer2.weight.shape, experts[0].layer2.weight\n",
        "# the shapes are transposed because that's just how nn.Linear likes to store them"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuzinXphaSU8",
        "outputId": "db4eba65-7876-4bbd-c226-12a89e21bd95"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 8]),\n",
              " Parameter containing:\n",
              " tensor([[ 0.1813,  0.2074, -0.2251, -0.1164,  0.3516,  0.1954,  0.1205, -0.3437],\n",
              "         [-0.0579, -0.2692,  0.0008, -0.0491, -0.1541,  0.2428, -0.0491, -0.1876],\n",
              "         [ 0.2132, -0.1592,  0.0324, -0.2014, -0.3443, -0.1662,  0.1397, -0.1477],\n",
              "         [ 0.2545,  0.2586,  0.1902, -0.0009,  0.0110,  0.1871,  0.0629,  0.1224],\n",
              "         [ 0.3390, -0.3366, -0.2746, -0.3149,  0.2216,  0.2643, -0.0534,  0.0814],\n",
              "         [-0.2345, -0.0323, -0.3052, -0.1905,  0.1901,  0.2788,  0.0012, -0.2929],\n",
              "         [ 0.2573,  0.2028, -0.2127,  0.1267, -0.0411,  0.0648, -0.2715,  0.0570],\n",
              "         [-0.1965, -0.2691, -0.2221,  0.3032,  0.1624,  0.0995, -0.2189, -0.2201],\n",
              "         [ 0.0474,  0.3065, -0.3530,  0.3520,  0.1216,  0.1862,  0.1653,  0.2844],\n",
              "         [-0.0030,  0.2865, -0.2152, -0.1480,  0.2431,  0.2322, -0.0958, -0.1910],\n",
              "         [ 0.2340,  0.2839,  0.1826, -0.0306,  0.1063,  0.3527, -0.1548, -0.3126],\n",
              "         [ 0.1623, -0.1185,  0.1744,  0.0576,  0.1349,  0.2969,  0.3458, -0.0014],\n",
              "         [-0.0355, -0.1128,  0.0958,  0.0686,  0.1919, -0.1776, -0.0985, -0.0941],\n",
              "         [-0.0540, -0.1798,  0.1804,  0.0485,  0.0172,  0.3503, -0.1021, -0.0691],\n",
              "         [ 0.2683,  0.2777,  0.1968, -0.2602, -0.3342, -0.2683,  0.2019, -0.2292],\n",
              "         [ 0.3521,  0.0183, -0.3354, -0.1558, -0.1471, -0.0908,  0.0396, -0.1691],\n",
              "         [ 0.2590,  0.0149, -0.0500, -0.0590, -0.2275, -0.3262, -0.2389,  0.3071],\n",
              "         [ 0.2195, -0.0751, -0.0152, -0.3529, -0.1359, -0.2098, -0.2857,  0.2190],\n",
              "         [ 0.3033,  0.2012, -0.0984,  0.0589,  0.2782,  0.0189, -0.0813,  0.2038],\n",
              "         [-0.0507,  0.3504,  0.2804,  0.0072,  0.1782,  0.2393,  0.2380, -0.2454],\n",
              "         [-0.2624, -0.0428, -0.0935,  0.0318,  0.0837,  0.2922,  0.1195, -0.1021],\n",
              "         [-0.0625, -0.1304, -0.3236,  0.2737,  0.0369, -0.2527, -0.2998,  0.1409],\n",
              "         [-0.2218,  0.0127, -0.0994,  0.1179,  0.3459, -0.2796, -0.0686,  0.1339],\n",
              "         [ 0.2305, -0.2003, -0.0599,  0.0276,  0.1588,  0.2623, -0.1127,  0.2467],\n",
              "         [ 0.0634,  0.2311,  0.1371, -0.2734,  0.0462, -0.3088, -0.1140, -0.0407],\n",
              "         [-0.0576,  0.1639, -0.0333, -0.2339, -0.2464, -0.1612, -0.1020,  0.2402],\n",
              "         [-0.3044, -0.2854, -0.0265, -0.2038, -0.2586,  0.2246, -0.1733,  0.3295],\n",
              "         [-0.3375,  0.1492, -0.3308,  0.0994,  0.3052, -0.1102, -0.2309, -0.0342],\n",
              "         [-0.2790,  0.0393,  0.1153, -0.2657, -0.2780, -0.1611,  0.0230,  0.3307],\n",
              "         [-0.0155,  0.3389,  0.0438, -0.1719, -0.3187,  0.3048, -0.2470,  0.2341],\n",
              "         [ 0.0035, -0.0033,  0.2732,  0.0573, -0.2936, -0.2659,  0.1378,  0.2372],\n",
              "         [ 0.2746,  0.2682, -0.2962,  0.0461, -0.1946, -0.0971,  0.1818, -0.2335]],\n",
              "        requires_grad=True),\n",
              " torch.Size([8, 16]),\n",
              " Parameter containing:\n",
              " tensor([[-0.2331, -0.0943, -0.0595,  0.0394,  0.1235,  0.0907,  0.1824,  0.0303,\n",
              "          -0.0781,  0.1075, -0.1436, -0.0916, -0.2230, -0.0221, -0.2362,  0.2241],\n",
              "         [ 0.2197, -0.2316, -0.0907,  0.2470,  0.1034,  0.0852, -0.0190, -0.0799,\n",
              "          -0.1998, -0.0707, -0.1436,  0.0026,  0.2326, -0.1946, -0.1792, -0.1204],\n",
              "         [-0.0182,  0.0389, -0.1282, -0.0545, -0.1104, -0.1459, -0.1165, -0.0685,\n",
              "          -0.2278,  0.0988,  0.0302, -0.0951, -0.2113,  0.1611,  0.2147,  0.1425],\n",
              "         [-0.1272, -0.1621,  0.0730, -0.1288,  0.1432,  0.0343, -0.1727, -0.0986,\n",
              "           0.2197,  0.1880, -0.2452, -0.2121, -0.2004, -0.2176,  0.0192, -0.2241],\n",
              "         [ 0.1609, -0.2228, -0.1486,  0.0578,  0.0687,  0.1859, -0.1275,  0.0405,\n",
              "          -0.2253,  0.0631,  0.0549,  0.1522, -0.0189,  0.1372,  0.1622,  0.2334],\n",
              "         [-0.1075,  0.0072, -0.2145, -0.1464,  0.0970,  0.1788, -0.0128, -0.1004,\n",
              "           0.0044, -0.1737, -0.2428,  0.0696,  0.1997,  0.0306, -0.2492,  0.2219],\n",
              "         [-0.0980, -0.0290, -0.0304,  0.2090, -0.1216, -0.1190,  0.0741,  0.2055,\n",
              "           0.0730, -0.2494,  0.0093,  0.1309, -0.0891,  0.2186, -0.2001,  0.1197],\n",
              "         [-0.0079, -0.0589,  0.1459,  0.1631, -0.1844, -0.0423,  0.0070, -0.0596,\n",
              "           0.1509, -0.1992,  0.1783,  0.0418, -0.1836, -0.1583, -0.1089, -0.1543]],\n",
              "        requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1h. Input-dependent routing\n",
        "<a id='h'></a>\n",
        "we choose which experts to utilize based on the characteristics of the input (x_normed). It's really just a linear layer with output dimension equal to our toal number of experts"
      ],
      "metadata": {
        "id": "b6HULuicNYZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we define the router that chooses which experts get used, which is just a simple linear layer with output size = tot_num_experts\n",
        "router = nn.Linear(d, tot_num_experts, bias=False)\n",
        "router.weight.shape, router.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS6wht98aSXv",
        "outputId": "19019a4c-97ac-42f3-fbf1-60b53069cd6d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 8]),\n",
              " Parameter containing:\n",
              " tensor([[ 0.1603, -0.2155, -0.2352, -0.0751,  0.0729,  0.1542, -0.3144, -0.0696],\n",
              "         [-0.1304,  0.1343, -0.2280,  0.2235, -0.2459,  0.2878,  0.0018, -0.2801],\n",
              "         [ 0.2473,  0.1879,  0.2609,  0.1022,  0.2676,  0.2690,  0.1390, -0.3515],\n",
              "         [-0.2119,  0.0607, -0.2558, -0.0393, -0.0642, -0.3025, -0.2477, -0.0958]],\n",
              "        requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The router is data dependent which is why we can backpropogate gradients through it\n",
        "x_routed = router(x_normed)\n",
        "x_routed.shape, x_routed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDgMOlvPceMc",
        "outputId": "b2ab5439-12c8-480d-d784-5cb8d010c037"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 4]),\n",
              " tensor([[[-0.2819,  0.3834,  0.5945, -0.2692],\n",
              "          [ 0.0401, -0.1393,  0.6424, -1.0065],\n",
              "          [-0.6340,  1.2705, -0.2847,  0.2607],\n",
              "          [ 0.2403,  0.3586, -0.4244,  0.2555],\n",
              "          [-0.2143, -0.9461,  0.7128, -1.2180]]], grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here we softmax to get the model's probabilities denoting which experts it things we should use\n",
        "# this step isn't strictly necessary for doing topK later but it helps smooth out the training\n",
        "routing_probs = F.softmax(x_routed, dim=-1)\n",
        "routing_probs.shape, routing_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcxSbK67ceO7",
        "outputId": "ca6e42f7-aa8c-46a5-8f99-6e0729ed63cc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 4]),\n",
              " tensor([[[0.1572, 0.3058, 0.3777, 0.1592],\n",
              "          [0.2492, 0.2083, 0.4551, 0.0875],\n",
              "          [0.0863, 0.5799, 0.1225, 0.2113],\n",
              "          [0.2736, 0.3079, 0.1407, 0.2778],\n",
              "          [0.2286, 0.1100, 0.5777, 0.0838]]], grad_fn=<SoftmaxBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here we'll select our top-k expert probabilities and indices\n",
        "# notice how the experts actualy act on a per-token in the sequence level, so in reality all of them are likely to be active to some extent\n",
        "expert_gate, expert_indices = torch.topk(routing_probs, k = chosen_num_experts, sorted=True)\n",
        "expert_gate, expert_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1kNNQTpceXT",
        "outputId": "132e721e-efe1-4c15-d21a-7ab532c4d26c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.3777, 0.3058],\n",
              "          [0.4551, 0.2492],\n",
              "          [0.5799, 0.2113],\n",
              "          [0.3079, 0.2778],\n",
              "          [0.5777, 0.2286]]], grad_fn=<TopkBackward0>),\n",
              " tensor([[[2, 1],\n",
              "          [2, 0],\n",
              "          [1, 3],\n",
              "          [1, 3],\n",
              "          [2, 0]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1i. Letting our experts do their thing\n",
        "<a id='i'></a>"
      ],
      "metadata": {
        "id": "asvBxg83N_P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape x_normed to (b*seq_len, d) for batched processing\n",
        "x_reshaped = x_normed.view(-1, d)\n",
        "x_reshaped.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTNHvh4kHz_P",
        "outputId": "e7dba0b0-ebcb-4c05-db43-33f7f556b427"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply all experts to the input\n",
        "expert_outputs = [expert(x_reshaped) for expert in experts]\n",
        "expert_outputs[0].shape # output shape of the first expert. the rest look the same"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-MBaNYFIHKA",
        "outputId": "b1de8abd-4fda-4a6f-9ada-29e2027dda51"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the expert outputs\n",
        "expert_outputs_concat = torch.cat(expert_outputs, dim=0)\n",
        "expert_outputs_concat.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItYPgCSnJW9P",
        "outputId": "3b198c84-6ca7-4bf2-9111-07156a1b9f8c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then reshape for masking\n",
        "expert_outputs_reshaped = expert_outputs_concat.view(b, seq_len, tot_num_experts, d)\n",
        "expert_outputs_reshaped.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW2GFKPYJW_p",
        "outputId": "c4d7913d-f42d-4e12-973d-74eb8063c67c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1j. Applying our router to the output of our experts\n",
        "<a id='j'></a>"
      ],
      "metadata": {
        "id": "Fa54Q3bSOSeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here we turn out expert_indices into multi-hot vectors to make them compatible with our module list of experts\n",
        "multi_hot_indices = torch.zeros(b, seq_len, tot_num_experts)\n",
        "multi_hot_indices.scatter_(2, expert_indices, 1)\n",
        "multi_hot_indices.shape, multi_hot_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hfL0BPlbcn",
        "outputId": "bc1dc89a-88a9-4391-e706-c9e6c2bec082"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 4]),\n",
              " tensor([[[0., 1., 1., 0.],\n",
              "          [1., 0., 1., 0.],\n",
              "          [0., 1., 0., 1.],\n",
              "          [0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0.]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the multi-hot mask (first expand dimensions for broadcasting)\n",
        "multi_hot_expanded = multi_hot_indices.unsqueeze(-1).expand_as(expert_outputs_reshaped)\n",
        "output_masked = expert_outputs_reshaped * multi_hot_expanded.float()\n",
        "output_masked.shape, output_masked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XyzwFIXJXCg",
        "outputId": "5aa669f7-9fc2-409f-87cf-08439c5c7cdd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 4, 8]),\n",
              " tensor([[[[ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
              "            -0.0000],\n",
              "           [ 0.0909,  0.0084,  0.0216,  0.0095,  0.0201,  0.0292,  0.0187,\n",
              "            -0.0400],\n",
              "           [-0.0697, -0.0328,  0.0885, -0.0423, -0.0131, -0.0808,  0.0357,\n",
              "             0.0964],\n",
              "           [ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
              "             0.0000]],\n",
              " \n",
              "          [[-0.0166,  0.0360,  0.0348,  0.0593, -0.0237, -0.0491, -0.1124,\n",
              "             0.0489],\n",
              "           [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
              "            -0.0000],\n",
              "           [ 0.0085,  0.1057,  0.0294,  0.0416,  0.0547, -0.0249, -0.0197,\n",
              "            -0.0440],\n",
              "           [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
              "            -0.0000]],\n",
              " \n",
              "          [[ 0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
              "             0.0000],\n",
              "           [-0.0940, -0.1789, -0.0888, -0.0640,  0.0995,  0.1528, -0.1626,\n",
              "             0.0427],\n",
              "           [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
              "            -0.0000],\n",
              "           [ 0.0322,  0.0337,  0.0331, -0.1380,  0.0136, -0.0169,  0.0511,\n",
              "             0.0066]],\n",
              " \n",
              "          [[ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
              "            -0.0000],\n",
              "           [ 0.0553,  0.2017,  0.0232, -0.1818, -0.0308,  0.1423,  0.2612,\n",
              "            -0.1972],\n",
              "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
              "            -0.0000],\n",
              "           [ 0.0134,  0.1203, -0.0061, -0.0845, -0.1310, -0.1272, -0.0478,\n",
              "            -0.1410]],\n",
              " \n",
              "          [[-0.0183, -0.0348,  0.0896, -0.0506,  0.0614,  0.0389, -0.0214,\n",
              "            -0.1825],\n",
              "           [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
              "             0.0000],\n",
              "           [-0.1919,  0.0811, -0.0468,  0.0672, -0.0202, -0.1084,  0.0998,\n",
              "            -0.1070],\n",
              "           [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
              "            -0.0000]]]], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then weight our experts' outputs by the softmax values (which we first must broadcast to the right shape)\n",
        "# this step is important because it allows gradients to backprop through the router, meaning the model can learn which experts to use\n",
        "routing_probs = routing_probs.unsqueeze(-1).expand_as(output_masked)\n",
        "MoE_output = output_masked * routing_probs\n",
        "MoE_output.shape, MoE_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3JMvf5bL9aP",
        "outputId": "76d00415-e3f7-49e5-e67b-1cd59f31bed5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 4, 8]),\n",
              " tensor([[[[ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
              "            -0.0000],\n",
              "           [ 0.0278,  0.0026,  0.0066,  0.0029,  0.0062,  0.0089,  0.0057,\n",
              "            -0.0122],\n",
              "           [-0.0263, -0.0124,  0.0334, -0.0160, -0.0049, -0.0305,  0.0135,\n",
              "             0.0364],\n",
              "           [ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
              "             0.0000]],\n",
              " \n",
              "          [[-0.0041,  0.0090,  0.0087,  0.0148, -0.0059, -0.0122, -0.0280,\n",
              "             0.0122],\n",
              "           [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
              "            -0.0000],\n",
              "           [ 0.0039,  0.0481,  0.0134,  0.0189,  0.0249, -0.0113, -0.0089,\n",
              "            -0.0200],\n",
              "           [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
              "            -0.0000]],\n",
              " \n",
              "          [[ 0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
              "             0.0000],\n",
              "           [-0.0545, -0.1038, -0.0515, -0.0371,  0.0577,  0.0886, -0.0943,\n",
              "             0.0247],\n",
              "           [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
              "            -0.0000],\n",
              "           [ 0.0068,  0.0071,  0.0070, -0.0291,  0.0029, -0.0036,  0.0108,\n",
              "             0.0014]],\n",
              " \n",
              "          [[ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
              "            -0.0000],\n",
              "           [ 0.0170,  0.0621,  0.0072, -0.0560, -0.0095,  0.0438,  0.0804,\n",
              "            -0.0607],\n",
              "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
              "            -0.0000],\n",
              "           [ 0.0037,  0.0334, -0.0017, -0.0235, -0.0364, -0.0353, -0.0133,\n",
              "            -0.0392]],\n",
              " \n",
              "          [[-0.0042, -0.0080,  0.0205, -0.0116,  0.0140,  0.0089, -0.0049,\n",
              "            -0.0417],\n",
              "           [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
              "             0.0000],\n",
              "           [-0.1108,  0.0468, -0.0271,  0.0388, -0.0117, -0.0626,  0.0576,\n",
              "            -0.0618],\n",
              "           [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
              "            -0.0000]]]], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally sum across the chosen experts\n",
        "MoE_output = MoE_output.sum(dim=2)\n",
        "MoE_output.shape, MoE_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYSMysgkMv-U",
        "outputId": "7a56bc74-8991-48a1-8a90-1e1d77a45937"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 8]),\n",
              " tensor([[[ 0.0015, -0.0098,  0.0400, -0.0131,  0.0012, -0.0216,  0.0192,\n",
              "            0.0242],\n",
              "          [-0.0003,  0.0571,  0.0221,  0.0337,  0.0190, -0.0236, -0.0370,\n",
              "           -0.0078],\n",
              "          [-0.0477, -0.0967, -0.0445, -0.0663,  0.0606,  0.0850, -0.0835,\n",
              "            0.0261],\n",
              "          [ 0.0207,  0.0955,  0.0055, -0.0794, -0.0459,  0.0085,  0.0672,\n",
              "           -0.0999],\n",
              "          [-0.1150,  0.0389, -0.0066,  0.0272,  0.0024, -0.0537,  0.0528,\n",
              "           -0.1035]]], grad_fn=<SumBackward1>))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1k. Our final residual connection\n",
        "<a id='k'></a>"
      ],
      "metadata": {
        "id": "VZjmHXsuObWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "post_moe_norm = RMSNorm(d)\n",
        "x += post_moe_norm(MoE_output)"
      ],
      "metadata": {
        "id": "Aal1G2gpmp3s"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1l. Output\n",
        "<a id='l'></a>\n",
        "So usually we'd run it back on steps 1b through 1k for however many layers our model has (Grok uses 64) but I do't feel like doing all that for this guide. Since our current `x` is of the same shape that it would be if we were to do more layers, let's go ahead and just see what Grok's output mechanism looks like. It's nothing interesting though, we're just reusing the embedding matrix to get our final logits"
      ],
      "metadata": {
        "id": "9uQuAb91PaQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiply x by the transpose of the embedding weights to get our final output logits\n",
        "logits = x @ embedding.weight.t()\n",
        "logits.shape, logits"
      ],
      "metadata": {
        "id": "4Do_Yyppmp6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7fd6d0-5c9b-46f4-98b8-4cafa6cb5f45"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 10]),\n",
              " tensor([[[ 2.6314e+01, -3.3829e+00, -3.6739e+00,  5.8809e+00, -1.7053e+01,\n",
              "            6.0706e+01,  8.2378e+00,  2.4363e+01,  1.3257e+01,  6.3304e+00],\n",
              "          [ 2.0947e+01,  1.6146e+01, -3.7018e+00,  5.9428e+00,  3.0899e+00,\n",
              "           -9.7007e+00, -1.0007e+00, -8.6589e+00,  1.4192e+00, -1.3098e+00],\n",
              "          [-5.4464e+00,  1.9512e-02,  1.0307e+01, -1.6062e+00,  1.1201e-02,\n",
              "            4.8058e-01, -6.8563e+00,  3.9441e+00, -1.2325e+01,  3.1033e+00],\n",
              "          [ 3.6239e+00, -1.3197e+00,  2.2565e+00,  2.2718e+00, -2.6819e+00,\n",
              "            1.5267e+01,  4.1053e+00,  1.0179e+01, -4.7458e+00,  4.5749e+00],\n",
              "          [ 2.0070e+01,  5.3797e+00, -1.1783e+01,  1.4306e+00, -7.5715e+00,\n",
              "            1.5827e+01, -3.3211e+00, -6.6944e-01,  2.7861e+01, -5.8105e+00]]],\n",
              "        grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax the logits\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "probs"
      ],
      "metadata": {
        "id": "IWQu_ua-lbpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d600a05a-d443-402e-b035-308466d72930"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.1585e-15, 1.4677e-28, 1.0972e-28, 1.5484e-24, 1.6972e-34,\n",
              "          1.0000e+00, 1.6348e-23, 1.6457e-16, 2.4730e-21, 2.4271e-24],\n",
              "         [9.9184e-01, 8.1551e-03, 1.9564e-11, 3.0203e-07, 1.7420e-08,\n",
              "          4.8551e-14, 2.9145e-10, 1.3760e-13, 3.2770e-09, 2.1395e-10],\n",
              "         [1.4366e-07, 3.3973e-05, 9.9741e-01, 6.6847e-06, 3.3691e-05,\n",
              "          5.3873e-05, 3.5074e-08, 1.7202e-03, 1.4795e-10, 7.4198e-04],\n",
              "         [8.7292e-06, 6.2230e-08, 2.2240e-06, 2.2583e-06, 1.5937e-08,\n",
              "          9.9382e-01, 1.4127e-05, 6.1345e-03, 2.0233e-09, 2.2594e-05],\n",
              "         [4.1342e-04, 1.7240e-10, 6.0653e-18, 3.3223e-12, 4.0918e-16,\n",
              "          5.9419e-06, 2.8695e-14, 4.0684e-13, 9.9958e-01, 2.3805e-15]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedily decode the probabilities to get our final predicted indices\n",
        "greedy_indices = torch.argmax(probs, dim=-1)\n",
        "greedy_indices\n"
      ],
      "metadata": {
        "id": "Z-5_nzJXlbsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6a75fc-b90e-406e-fae9-df4f4d33a2a8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5, 0, 2, 5, 8]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and that's it! those are all the essentail calcuations that Grok performs, most of which aren't any different from other open-source LLMs like Llama, Mistral or Gemini (Grok is most similar to Mistral which also has an MoE while the other two do not). Now let's code everything up the correct way into classes so that we can actually build a functioning model\n",
        "\n",
        "# 2. Actually functional model code\n",
        "<a id='two'></a>\n",
        "The bulk of the lesson is over, but the following code demosntrates how you'd actually take the concepts and turn them into functioning nn.Module classes. Alternatively to reading through them here, you can check out the .py files in [the repo](https://github.com/evintunador/minGrok). I'm not going to bother explaining any of this section"
      ],
      "metadata": {
        "id": "5skJdzNNTXzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "suAJ3161DTpz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2a. Multi-query attention\n",
        "<a id='twoa'></a>"
      ],
      "metadata": {
        "id": "CauGV8VtxJ5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rotary_emb(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
        "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
        "    # Get sequence length\n",
        "    seq_len = x.size(1)\n",
        "    device = x.device\n",
        "\n",
        "    # Dynamically compute frequency cis based on the input sequence length\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
        "    t = torch.arange(seq_len, device=device)\n",
        "    freqs = torch.outer(t, freqs).float()\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "\n",
        "    # Apply rotary embeddings to the input tensor\n",
        "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
        "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
        "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
        "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
        "\n",
        "    return x_out\n",
        "\n",
        "class MQA(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements Multi-Query Attention which supports a distinct number of attention heads for queries and key-values (KV).\n",
        "    In the case where the same number of queries and key-values are used, this implemenation is equivalent to regular Multi-Head Attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.num_kv_heads = config.num_key_value_heads\n",
        "        assert self.num_heads % self.num_kv_heads == 0\n",
        "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.head_dim = config.head_dim\n",
        "        self.theta = config.rope_theta\n",
        "\n",
        "        self.q_size = self.num_heads * self.head_dim\n",
        "        self.kv_size = self.num_kv_heads * self.head_dim\n",
        "\n",
        "        self.qkv_proj = nn.Linear(self.hidden_size, (self.num_heads + 2 * self.num_kv_heads) * self.head_dim, bias=False)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
        "\n",
        "        # Create a mask tensor with shape [batch_size, num_heads, seq_len, seq_len]\n",
        "        self.mask = torch.tril(torch.ones((config.max_position_embeddings, config.max_position_embeddings),\n",
        "                                     dtype=torch.uint8)).view(1, 1, config.max_position_embeddings, config.max_position_embeddings).to(dtype=torch.bool)\n",
        "        #self.mask = mask.expand(-1, self.num_heads, -1, -1)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states_shape = hidden_states.shape\n",
        "        assert len(hidden_states_shape) == 3\n",
        "        batch_size, input_len, _ = hidden_states_shape\n",
        "\n",
        "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
        "        qkv = self.qkv_proj(hidden_states)\n",
        "        xq, xk, xv = qkv.split([self.q_size, self.kv_size, self.kv_size],dim=-1)\n",
        "\n",
        "        # Reshapes each to separate the heads and align the dimensions for attention operations.\n",
        "        xq = xq.view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        xk = xk.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
        "        xv = xv.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
        "\n",
        "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
        "        xq = apply_rotary_emb(xq, self.head_dim, self.theta)\n",
        "        xk = apply_rotary_emb(xk, self.head_dim, self.theta)\n",
        "\n",
        "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
        "        if self.num_kv_heads != self.num_heads:\n",
        "            xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n",
        "            xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
        "\n",
        "        # Transposes to align them for the batch matrix multiplication in attention calculation.\n",
        "        q = xq.transpose(1, 2)\n",
        "        k = xk.transpose(1, 2)\n",
        "        v = xv.transpose(1, 2)\n",
        "\n",
        "        # Calculates attention logits by performing a batch matrix multiplication between queries and keys\n",
        "        logits = torch.matmul(q, k.transpose(2, 3))\n",
        "\n",
        "        # Grok's unusual scaling method\n",
        "        # If anyone knows why they use 0.08838834764831845 in Grok please lmk. Maybe it's a learned value?\n",
        "        logits *= 0.08838834764831845\n",
        "        # Next here we'll scale and clip our attention logits\n",
        "        # the tanh is a nonlinear function that pushes all of the entries in logits into the range (-1, 1)\n",
        "        # then they're scaled up to the range (-30, 30). The number 30 is an arbitrary choice\n",
        "        # the purpose of this scaling is to regularize and prevent numerical stability that might otherwise mess with the upcoming softmax\n",
        "        max_attn_val = torch.tensor(30.0, dtype = logits.dtype)\n",
        "        logits = max_attn_val * torch.tanh(logits / max_attn_val)\n",
        "        # other transformers would replace the last three lines with a multiplication by torch.sqrt(self.hidden_size)\n",
        "\n",
        "        # Applies the lower-triangular mask to the attention logits\n",
        "        logits = torch.where(self.mask[..., :input_len, :input_len].expand_as(logits), logits, torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
        "\n",
        "        # Applies softmax to the logits to obtain attention probabilities\n",
        "        scores = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
        "        output = torch.matmul(scores, v)\n",
        "\n",
        "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)\n",
        "\n",
        "        # Applies the final linear projection to the attention output, mapping it back to the hidden size dimension.\n",
        "        output = self.o_proj(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "9bJh8MHTUp7j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2b. Mixture-of-Experts\n",
        "<a id='twob'></a>"
      ],
      "metadata": {
        "id": "HW3uAiv-xQWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Expert(nn.Module):\n",
        "    def __init__(self, model_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(model_dim, hidden_dim * 2, bias=False)  # Double the output for gating\n",
        "        self.layer2 = nn.Linear(hidden_dim, model_dim, bias=False)  # Output layer remains the same\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Split the output of the first layer for gating\n",
        "        x, gate = self.layer1(x).chunk(2, dim=-1)\n",
        "\n",
        "        # Apply GeLU to the gate, and then multiply element-wise\n",
        "        x = F.gelu(gate) * x\n",
        "        x = self.layer2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Router(nn.Module):\n",
        "    def __init__(self, input_size, tot_num_experts):\n",
        "        super().__init__()\n",
        "        self.tot_num_experts = tot_num_experts\n",
        "        self.router_weights = nn.Linear(input_size, tot_num_experts, bias=False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        routing_logits = self.router_weights(inputs)\n",
        "        routing_probs = F.softmax(routing_logits, dim=-1)\n",
        "        return routing_probs\n",
        "\n",
        "class MoELayer(nn.Module):\n",
        "    def __init__(self, model_dim, expert_hidden_dim, tot_num_experts, chosen_num_experts):\n",
        "        super().__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.tot_num_experts = tot_num_experts\n",
        "        self.chosen_num_experts = chosen_num_experts\n",
        "        self.experts = nn.ModuleList([Expert(model_dim, expert_hidden_dim) for _ in range(tot_num_experts)])\n",
        "        self.router = Router(model_dim, tot_num_experts)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        b, seq_len, _ = inputs.shape\n",
        "\n",
        "        # get the output of all the experts\n",
        "        expert_outputs = [expert(inputs.view(-1, self.model_dim)) for expert in self.experts]\n",
        "        expert_outputs = torch.cat(expert_outputs, dim=0).view(b, seq_len, self.tot_num_experts, self.model_dim)\n",
        "\n",
        "        # get the output of the router and create out expert mask\n",
        "        routing_probs = F.softmax(self.router(inputs), dim=-1)\n",
        "        with torch.no_grad():\n",
        "          expert_indices = torch.topk(routing_probs, k=self.chosen_num_experts, sorted=True).indices\n",
        "          multi_hot_indices = torch.zeros(b, seq_len, self.tot_num_experts, device=inputs.device)\n",
        "          multi_hot_indices = multi_hot_indices.scatter(2, expert_indices, 1)\n",
        "\n",
        "        # Apply the multi-hot mask (first expand dimensions for broadcasting)\n",
        "        multi_hot_expanded = multi_hot_indices.unsqueeze(-1).expand_as(expert_outputs)\n",
        "        output_masked = expert_outputs * multi_hot_expanded.float()\n",
        "\n",
        "        # then weight our experts' outputs by the softmax values (which we first must broadcast to the right shape) and sum them\n",
        "        routing_probs = routing_probs.unsqueeze(-1).expand_as(output_masked)\n",
        "        MoE_output = (output_masked * routing_probs).sum(dim=2)\n",
        "\n",
        "        return MoE_output"
      ],
      "metadata": {
        "id": "CEf5HsU4UqBf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2c. Residual Layers\n",
        "<a id='twoc'></a>"
      ],
      "metadata": {
        "id": "xF9wBRbNxUTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module): # the same RMSNorm we wrote earlier\n",
        "    def __init__(self, num_features, eps=1e-5, use_scale=True):\n",
        "        super(RMSNorm, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = nn.Parameter(torch.ones(num_features)) if use_scale else None\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Calculate the mean squared value for each feature\n",
        "        mean_squared = inputs.pow(2).mean(dim=-1, keepdim=True)\n",
        "\n",
        "        # Normalize inputs\n",
        "        normed_inputs = inputs * torch.rsqrt(mean_squared + self.eps)\n",
        "\n",
        "        # Apply scale if it exists\n",
        "        if self.scale is not None:\n",
        "            normed_inputs = normed_inputs * self.scale\n",
        "\n",
        "        return normed_inputs\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A decoder layer that integrates the Attention mechanism and MoE. It includes\n",
        "    normalization steps both before and after the MQA and MoE but never actually normalized the residual connection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mqa = MQA(config)\n",
        "\n",
        "        self.moe = MoELayer(\n",
        "            model_dim = config.hidden_size,\n",
        "            expert_hidden_dim = config.hidden_size * config.embedding_multiplier_scale,\n",
        "            tot_num_experts = config.tot_num_experts,\n",
        "            chosen_num_experts = config.chosen_num_experts\n",
        "        )\n",
        "\n",
        "        self.pre_mqa_norm = RMSNorm(config.hidden_size, eps = config.rms_norm_eps, use_scale = config.use_scale)\n",
        "        self.post_mqa_norm = RMSNorm(config.hidden_size, eps = config.rms_norm_eps, use_scale = config.use_scale)\n",
        "        self.pre_moe_norm = RMSNorm(config.hidden_size, eps = config.rms_norm_eps, use_scale = config.use_scale)\n",
        "        self.post_moe_norm = RMSNorm(config.hidden_size, eps = config.rms_norm_eps, use_scale = config.use_scale)\n",
        "\n",
        "        self.drop = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
        "        if training:\n",
        "            x = x + self.drop(self.post_mqa_norm(self.mqa(self.pre_mqa_norm(x))))\n",
        "            x = x + self.drop(self.post_moe_norm(self.moe(self.pre_moe_norm(x))))\n",
        "        else:\n",
        "            x = x + self.post_mqa_norm(self.mqa(self.pre_mqa_norm(x)))\n",
        "            x = x + self.post_moe_norm(self.moe(self.pre_moe_norm(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "nvfkGSrnUqFY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2d. The model itself\n",
        "<a id='twod'></a>"
      ],
      "metadata": {
        "id": "xedETQt7xark"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class minGrok(nn.Module):\n",
        "\n",
        "    def __init__(self, config, tokenizer):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # the attention heads need to cleanly divide up the hidden_size of the model so that we can split it all apart & combine back together\n",
        "        assert config.hidden_size % config.num_attention_heads == 0\n",
        "\n",
        "        self.max_seq_len = config.max_position_embeddings\n",
        "        self.head_dim = config.head_dim\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "         # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
        "        self.embedder = nn.Embedding(self.vocab_size, config.hidden_size)\n",
        "\n",
        "        # Initialize a sequence of DecoderLayer instances as specified by the number of layers in the config\n",
        "        self.layers = nn.ModuleList(DecoderLayer(config) for _ in range(config.num_layers))\n",
        "\n",
        "        # Initialize a normalization layer to be applied after the last decoder layer, stabilizing the output\n",
        "        self.final_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "        # the loss function\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len) list of integer token ids\n",
        "        target_token_ids: torch.Tensor = None, # a shape (batch_size, input_seq_len) list of token ids to train on\n",
        "        ) -> torch.Tensor:\n",
        "\n",
        "        # turn the input tokens into the first resudial state using the embedding matrix\n",
        "        x = self.embedder(input_token_ids) * self.config.hidden_size**0.5 # Grok normalizes the embedding by sqrt(hidden_size)\n",
        "\n",
        "        # Iteratively process the input through each DecoderLayer\n",
        "        for i in range(len(self.layers)):\n",
        "            layer = self.layers[i]\n",
        "            x = layer(x, training=True) if target_token_ids is not None else layer(x, training=False)\n",
        "\n",
        "        # Apply normalization to the output of the final decoder layer\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        # grabbing the weights of the embedding matrix shape (vocab_size, hidden_dim) for use as the output layer\n",
        "        embedder_weight = self.embedder.weight\n",
        "\n",
        "        # the embedding matrix is also used as the output layer\n",
        "        # this saves on parameters & makes sense for interpretability\n",
        "        # (batch_size, input_len, hidden_size) @ (hidden_size, vocab_size) -> (batch_size, input_len, vocab_size)\n",
        "        logits = torch.matmul(x, embedder_weight.t())\n",
        "\n",
        "        if target_token_ids is None: # if we're not training, then we don't need to calculate loss\n",
        "            loss = None\n",
        "        else:\n",
        "            # if we are training\n",
        "            batch_size, input_len, vocab_size = logits.shape\n",
        "            # then we reshape our logits & targets before calculating cross-entropy loss\n",
        "            loss = self.criterion(logits.view(batch_size*input_len, vocab_size),\n",
        "                                  target_token_ids.view(batch_size*input_len))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad() # no need to keep track of gradients during inference\n",
        "    def Sampler(\n",
        "        self,\n",
        "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
        "        temperature: float, # controls how boring vs random the outputs should be\n",
        "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
        "        top_k: int, # the maximum number of output options we're willing to consider\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        The Sampler function is responsible for generating token predictions from Grok's output.\n",
        "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling\n",
        "        \"\"\"\n",
        "        # Select the last element for each sequence.\n",
        "        logits = logits[:,-1,:]\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        logits.div_(temperature) # div_ is an in-place operation which is ok since we don't record gradients during inference\n",
        "\n",
        "        # Calculate probabilities\n",
        "        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n",
        "\n",
        "        # sort the probabilities to for use in top-p & top-k\n",
        "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "\n",
        "        # calculating top_k\n",
        "        probs_sum = torch.cumsum(probs_sort, dim=-1) # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
        "        top_ps_mask = (probs_sum - probs_sort) > top_p # mask where 0's are top-p selections & 1's are to be excluded\n",
        "        probs_sort = torch.where(top_ps_mask, 0, probs_sort)  # the original probabilities with excluded tokens changed to 0.0\n",
        "\n",
        "        # calculating top_k\n",
        "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) # create a shape (vocab_size) tensor that just iterates up by 1's\n",
        "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1) # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
        "        top_ks_mask = top_ks_mask >= top_k # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
        "\n",
        "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
        "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
        "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True)) # Re-normalization so that total probabilities add up to 1\n",
        "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
        "        probs = torch.gather(probs_sort,\n",
        "                             dim=-1,\n",
        "                             index=torch.argsort(probs_idx, dim=-1))\n",
        "\n",
        "        # samples from the distribution\n",
        "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        return next_token_id\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        output_len: int = 100, # the model will output 100 tokens\n",
        "        temperature: float = 0.95, # 0.95 is pretty close to not even using temperature at all (1.0 would be no effect)\n",
        "        top_p: float = 1.0, # defaulting to 1 means we essentially don't use top-p\n",
        "        top_k: int = 65, # setting top_k = vocab_size means we're effectively not using top_k at all\n",
        "    ) -> str:\n",
        "        \"\"\"Generates responses for given prompts using Grok model.\"\"\"\n",
        "\n",
        "        # encoding the prompt into token indices\n",
        "        tokens = self.tokenizer.encode(prompt)\n",
        "\n",
        "        # turning it into the right tensor shape\n",
        "        tokens = torch.tensor(tokens, device=self.config.device).unsqueeze(0)\n",
        "\n",
        "        # we wouldn't want to go past the maximum context length we trained on\n",
        "        assert len(tokens) + output_len <= self.config.max_position_embeddings\n",
        "\n",
        "        for i in range(output_len):\n",
        "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
        "            logits, _ = self(tokens[:,:self.max_seq_len])\n",
        "\n",
        "            next_token = self.Sampler(\n",
        "                logits = logits, # the actual output of the model\n",
        "                temperature = temperature,\n",
        "                top_p = top_p,\n",
        "                top_k = top_k\n",
        "            )\n",
        "\n",
        "            # add our new token to the sequence\n",
        "            tokens = torch.cat((tokens, next_token), dim=1)\n",
        "\n",
        "        # decode our list of tokens to an actual string\n",
        "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "4gRVdIZVUqHi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train and test your own minGrok (or load mine)\n",
        "<a id='three'></a>\n",
        "\n",
        "### 3a. Setup\n",
        "<a id='threea'></a>\n",
        "a bunch of data, functions and objects you'll need that are not already included with the above architecture"
      ],
      "metadata": {
        "id": "NMw-8l3kUA5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the TinyShakespeare dataset\n",
        "!wget -O input.txt https://raw.githubusercontent.com/evintunador/minGrok/main/input.txt\n",
        "\n",
        "# load the dataset\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
        "print(text[:200])\n",
        "\n",
        "# here are all the unique characters that occur in this text and how many there are\n",
        "chars = sorted(list(set(text)))\n",
        "v = len(chars)\n",
        "print(chars)\n",
        "print(v)"
      ],
      "metadata": {
        "id": "5oTbAKuix5nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c754392-ad50-474e-f286-8030f0a79613"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-21 03:34:43--  https://raw.githubusercontent.com/evintunador/minGrok/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: input.txt\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  5.59MB/s    in 0.2s    \n",
            "\n",
            "2024-03-21 03:34:44 (5.59 MB/s) - input.txt saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the tokenizer code\n",
        "!wget https://raw.githubusercontent.com/evintunador/minGrok/main/tokenizer.py\n",
        "# and the tokenizer model\n",
        "!wget https://raw.githubusercontent.com/evintunador/minGrok/main/tokenizers/tokenizer.model\n",
        "!mkdir -p tokenizers\n",
        "!mv tokenizer.model tokenizers/\n",
        "\n",
        "from tokenizer import SimpleTokenizer, loaded_stoi, loaded_merges\n",
        "\n",
        "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
        "print(\"vocab length: \", tokenizer.vocab_len)\n",
        "\n",
        "# Encoding text\n",
        "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\n",
        "print(\"Encoded:\", encoded_text)\n",
        "\n",
        "# Decoding back\n",
        "decoded_text = tokenizer.decode(encoded_text)\n",
        "print(\"Decoded:\", decoded_text)"
      ],
      "metadata": {
        "id": "fnx2ACOizX3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3314f39-b10e-4410-8658-ed334a02ff5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-21 03:34:47--  https://raw.githubusercontent.com/evintunador/minGrok/main/tokenizer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2213 (2.2K) [text/plain]\n",
            "Saving to: tokenizer.py\n",
            "\n",
            "tokenizer.py        100%[===================>]   2.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-21 03:34:47 (34.4 MB/s) - tokenizer.py saved [2213/2213]\n",
            "\n",
            "--2024-03-21 03:34:47--  https://raw.githubusercontent.com/evintunador/minGrok/main/tokenizers/tokenizer.model\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 934 [application/octet-stream]\n",
            "Saving to: tokenizer.model\n",
            "\n",
            "tokenizer.model     100%[===================>]     934  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-21 03:34:48 (46.9 MB/s) - tokenizer.model saved [934/934]\n",
            "\n",
            "vocab length:  128\n",
            "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30, 53, 83, 53, 12]\n",
            "Decoded: JULIET:\n",
            "O Romeo, Romeo! wherefore art thou Romeo?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dataclasses\n",
        "from typing import Optional\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "    # v was defined earlier when we loaded TinyShakespeare. In Grok it's 131,072\n",
        "    vocab_size: int = tokenizer.vocab_len\n",
        "\n",
        "    # The maximum sequence length that this model might ever be used with.\n",
        "    max_position_embeddings: int = 256 # in Grok it's 8,192\n",
        "\n",
        "    # The number of blocks in the model.\n",
        "    num_layers: int = 4 # In Grok it's 64\n",
        "\n",
        "    # The number of attention heads used in the attention layers of the model.\n",
        "    num_attention_heads: int = 4 # In Grok it's 48\n",
        "\n",
        "    # The number of key-value heads for implementing attention.\n",
        "    num_key_value_heads: int = 1 # In Grok it's 8\n",
        "\n",
        "    # The hidden size of the model, AKA the embedding dimension. Each token embedding vector will be this long\n",
        "    hidden_size: int = 96 # In Grok it's 6,144\n",
        "\n",
        "    # How much wider should the inner dimension of the experts be than the model's embedding dimension?\n",
        "    embedding_multiplier_scale: int = 2 # In Grok it's 8\n",
        "\n",
        "    # how many experts?\n",
        "    tot_num_experts: int = 4 # in Grok it's 8\n",
        "\n",
        "    # how many active experts per token?\n",
        "    chosen_num_experts: int = 2 # in Grok it's also 2\n",
        "\n",
        "    # The number of head dimensions\n",
        "    head_dim: int = 24 # In Grok it's 128\n",
        "\n",
        "    # The epsilon used by the rms normalization layers.\n",
        "    rms_norm_eps: float = 1e-5 # this is to promote numerical stability & prevent dividing by 0\n",
        "\n",
        "    # the scaling factor that determines the frequencies for the rotary positional encodings\n",
        "    rope_theta = 100.0 # Grok and most models use 10,000\n",
        "    # smaller models should use a smaller theta, but I'm just guessing here. 1000 might work too\n",
        "\n",
        "    # whether to use a linear layer after normalization\n",
        "    use_scale: bool = True # same in Grok\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # the dropout rate to use during training\n",
        "    dropout = 0.05\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "_B7u9yQiUCPW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3b. Training your own\n",
        "<a id='threeb'></a>\n",
        "\n",
        "you can feel free to train your own if you'd like, but i don't see a huge reason to do so in a colab notebook"
      ],
      "metadata": {
        "id": "oHQNS8gdvZQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "I5hAq2VcvpdS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading for training which generates a small batch of data of inputs x and targets y\n",
        "def get_batch(split, batch_size):\n",
        "    # whether we grab from our training or validation dataset\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n",
        "    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n",
        "    x, y = x.to(config.device), y.to(config.device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "F6M2WCXs_Vld"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model, batch_size, eval_iters = 10): # to periodically estimate loss during the training loop\n",
        "    out = {}\n",
        "    model.eval() # sets model to eval mode\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, batch_size)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # just resets to training mode\n",
        "    return out"
      ],
      "metadata": {
        "id": "oaiM9_Od_Vnv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate a new model\n",
        "model = minGrok(config, tokenizer).to(config.device)\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pdPqc-L_VqU",
        "outputId": "2a1007d8-5867-4f9b-a12e-185cd2a0e508"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "992.352 K parameters\n",
            "minGrok(\n",
            "  (embedder): Embedding(128, 96)\n",
            "  (layers): ModuleList(\n",
            "    (0-3): 4 x DecoderLayer(\n",
            "      (mqa): MQA(\n",
            "        (qkv_proj): Linear(in_features=96, out_features=144, bias=False)\n",
            "        (o_proj): Linear(in_features=96, out_features=96, bias=False)\n",
            "      )\n",
            "      (moe): MoELayer(\n",
            "        (experts): ModuleList(\n",
            "          (0-3): 4 x Expert(\n",
            "            (layer1): Linear(in_features=96, out_features=384, bias=False)\n",
            "            (layer2): Linear(in_features=192, out_features=96, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (router): Router(\n",
            "          (router_weights): Linear(in_features=96, out_features=4, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (pre_mqa_norm): RMSNorm()\n",
            "      (post_mqa_norm): RMSNorm()\n",
            "      (pre_moe_norm): RMSNorm()\n",
            "      (post_moe_norm): RMSNorm()\n",
            "      (drop): Dropout(p=0.05, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_norm): RMSNorm()\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
        "learning_rate = 3e-4\n",
        "weight_decay = 0.01\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# how long we want to train for\n",
        "max_iters = 10 # make it a larger number (like 5000) to actually learn anything\n",
        "\n",
        "# how often we want to check & see how our loss is doing\n",
        "eval_interval = 2 # make it a larger number so that you don't spend all your time evaluating rather than training\n",
        "\n",
        "# batch size to use\n",
        "batch_size = 16\n",
        "\n",
        "import time as time"
      ],
      "metadata": {
        "id": "9KhAPH5g_VsX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
        "#torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', batch_size)\n",
        "\n",
        "    # train\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        current_time = time.time()\n",
        "        elapsed_time = current_time - start_time\n",
        "        losses = estimate_loss(model, batch_size)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Disable anomaly detection after the training loop\n",
        "#torch.autograd.set_detect_anomaly(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OECt3NLpBGKc",
        "outputId": "88eea193-7c3e-4563-ec45-18bdf3340631"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 91.3762, val loss 91.4219, time elapsed: 1.08 seconds\n",
            "step 2: train loss 89.1425, val loss 89.5488, time elapsed: 10.51 seconds\n",
            "step 4: train loss 87.3759, val loss 87.6508, time elapsed: 18.01 seconds\n",
            "step 6: train loss 85.3227, val loss 85.6378, time elapsed: 25.48 seconds\n",
            "step 8: train loss 83.4327, val loss 83.6638, time elapsed: 32.99 seconds\n",
            "step 9: train loss 82.5160, val loss 82.6469, time elapsed: 39.80 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3c. Alternatively, you can load the 1m parameter model I already trained\n",
        "<a id='threec'></a>"
      ],
      "metadata": {
        "id": "m3z_2iYbvqUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a blank model\n",
        "model = minGrok(config, tokenizer).to(config.device)\n",
        "\n",
        "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
        "path = 'models/minGrok-v128-max_t256-layers4-heads4-kv_heads1-hidden96-embedding_multiplier_scale2-head_dim24-theta100.0-lr0.0003-decay0.01-batch32-train_iter5000--2024-03-20_22-43-59.pth'\n",
        "\n",
        "# downloading it\n",
        "!wget https://raw.githubusercontent.com/evintunador/minGrok/main/models/minGrok-v128-max_t256-layers4-heads4-kv_heads1-hidden96-embedding_multiplier_scale2-head_dim24-theta100.0-lr0.0003-decay0.01-batch32-train_iter5000--2024-03-20_22-43-59.pth\n",
        "!mkdir -p models\n",
        "!mv minGrok-v128-max_t256-layers4-heads4-kv_heads1-hidden96-embedding_multiplier_scale2-head_dim24-theta100.0-lr0.0003-decay0.01-batch32-train_iter5000--2024-03-20_22-43-59.pth models/\n",
        "\n",
        "# Load the saved state dictionary\n",
        "model.load_state_dict(torch.load(path))\n",
        "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
        "\n",
        "# If you only plan to do inference, switch to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# If you plan to continue training the model, switch to training mode\n",
        "#model.train()"
      ],
      "metadata": {
        "id": "B2exYhJGvxDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a955a902-8bbb-4dba-920e-939b13fc7029"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-21 03:39:32--  https://raw.githubusercontent.com/evintunador/minGrok/main/models/minGrok-v128-max_t256-layers4-heads4-kv_heads1-hidden96-embedding_multiplier_scale2-head_dim24-theta100.0-lr0.0003-decay0.01-batch32-train_iter5000--2024-03-20_22-43-59.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4012574 (3.8M) [application/octet-stream]\n",
            "Saving to: minGrok-v128-max_t256-layers4-heads4-kv_heads1-hidden96-embedding_multiplier_scale2-head_dim24-theta100.0-lr0.0003-decay0.01-batch32-train_iter5000--2024-03-20_22-43-59.pth\n",
            "\n",
            "minGrok-v128-max_t2 100%[===================>]   3.83M  16.1MB/s    in 0.2s    \n",
            "\n",
            "2024-03-21 03:39:33 (16.1 MB/s) - minGrok-v128-max_t256-layers4-heads4-kv_heads1-hidden96-embedding_multiplier_scale2-head_dim24-theta100.0-lr0.0003-decay0.01-batch32-train_iter5000--2024-03-20_22-43-59.pth saved [4012574/4012574]\n",
            "\n",
            "992.352 K parameters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "minGrok(\n",
              "  (embedder): Embedding(128, 96)\n",
              "  (layers): ModuleList(\n",
              "    (0-3): 4 x DecoderLayer(\n",
              "      (mqa): MQA(\n",
              "        (qkv_proj): Linear(in_features=96, out_features=144, bias=False)\n",
              "        (o_proj): Linear(in_features=96, out_features=96, bias=False)\n",
              "      )\n",
              "      (moe): MoELayer(\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x Expert(\n",
              "            (layer1): Linear(in_features=96, out_features=384, bias=False)\n",
              "            (layer2): Linear(in_features=192, out_features=96, bias=False)\n",
              "          )\n",
              "        )\n",
              "        (router): Router(\n",
              "          (router_weights): Linear(in_features=96, out_features=4, bias=False)\n",
              "        )\n",
              "      )\n",
              "      (pre_mqa_norm): RMSNorm()\n",
              "      (post_mqa_norm): RMSNorm()\n",
              "      (pre_moe_norm): RMSNorm()\n",
              "      (post_moe_norm): RMSNorm()\n",
              "      (drop): Dropout(p=0.05, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): RMSNorm()\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3d. Testing (performing inference)\n",
        "<a id='threed'></a>"
      ],
      "metadata": {
        "id": "vFa4Pfi2vx3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\n",
        "max_useable_output_len = config.max_position_embeddings - len(input_str)\n",
        "output = model.generate(input_str, output_len = max_useable_output_len)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "rK5bkaFmv1dH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3b2788-c476-4c4c-efa6-4f1dd71c3028"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JULIET:\n",
            "O Romeo, Romeo! wherefore art thou be wand\n",
            "With reime my laper.\n",
            "Thunest guhin! in an was blace stay, I st makes come\n",
            "With fale and false of a her lade was rie'don is flanter to for?\n",
            "Where king of being myseter;\n",
            "What all is a now now so we your ather is dift:\n",
            "Ist grant the enout being Bume, for to be then a good degry go you se\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if it's not trained enough then chances are it'll output only spaces (the most common character) so this is just to confirm that it's outputting something\n",
        "len(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOlwC4iNQO87",
        "outputId": "decc7fcd-0633-4b90-fb83-8fe102a4b442"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hikwp10DQQEb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}